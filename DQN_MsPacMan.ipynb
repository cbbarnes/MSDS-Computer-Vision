{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN MsPacMan.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbbarnes/MSDS-Computer-Vision/blob/master/DQN_MsPacMan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsSGgH2Y0apl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#          _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Pacman Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z53K4nNuAmmN",
        "colab_type": "text"
      },
      "source": [
        "Define process function for input game screens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RInhWdEYAkxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "\n",
        "#prepro (210, 160, 3) uint8 frame into 7040 (88x80) 1D float vector \n",
        "\n",
        "def preprocess_observation(obs):\n",
        "\n",
        "    # Crop and resize the image\n",
        "    img = obs[1:176:2, ::2]\n",
        "\n",
        "    # Convert the image to greyscale\n",
        "    img = img.mean(axis=2)\n",
        "\n",
        "    # Improve image contrast\n",
        "    img[img==color] = 0\n",
        "\n",
        "    # Next we normalize the image from -1 to +1\n",
        "    img = (img - 128) / 128 - 1\n",
        "\n",
        "    return img.reshape(88,80,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h72NBtZqAto9",
        "colab_type": "text"
      },
      "source": [
        "Initialize game environment and look at observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Pacman!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHjI3m76K9w8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "state.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC1NdGY-LjNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"MsPacman-v0\")\n",
        "n_outputs = env.action_space.n\n",
        "print(n_outputs)\n",
        "print(env.env.get_action_meanings())\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for i in range(22):\n",
        "  \n",
        "  if i > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "\n",
        "  observation, _, _, _ = env.step(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdZIbPH0HJbW",
        "colab_type": "text"
      },
      "source": [
        "Look at original and preprocessed tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8HQIAcoHGUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs_preprocessed = preprocess_observation(observation).reshape(88,80)\n",
        "plt.imshow(obs_preprocessed)\n",
        "plt.show()\n",
        "print(observation.shape)\n",
        "print(obs_preprocessed.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tDJfjD2LlLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "reward, info, done = None, None, None\n",
        "while done != True:\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md3LeA60BGnH",
        "colab_type": "text"
      },
      "source": [
        "Define Deep Q network:\n",
        "3 Convolutional layers, 1 flattened and 1 dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4tLIEDtBDGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "#Reset is technically not necessary if variables done  in TF2\n",
        "#https://github.com/ageron/tf2_course/issues/8\n",
        "\n",
        "def q_network(X, name_scope):\n",
        "    \n",
        "    # Initialize layers\n",
        "    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name_scope) as scope: \n",
        "\n",
        "\n",
        "        # initialize the convolutional layers\n",
        "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
        "        tf.compat.v1.summary.histogram('layer_1',layer_1)\n",
        "        \n",
        "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_2',layer_2)\n",
        "        \n",
        "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_3',layer_3)\n",
        "        \n",
        "        # Flatten the result of layer_3 before feeding to the fully connected layer\n",
        "        flat = flatten(layer_3)\n",
        "        # Insert fully connected layer\n",
        "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('fc',fc)\n",
        "        #Add final output layer\n",
        "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('output',output)\n",
        "        \n",
        "\n",
        "        # Vars will store the parameters of the network such as weights\n",
        "        vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
        "        #Return both variables and outputs together\n",
        "        return vars, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUd6SeftBcsJ",
        "colab_type": "text"
      },
      "source": [
        "define epsilon greedy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzvVv7XJBbH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.5\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "\n",
        "#\n",
        "def epsilon_greedy(action, step):\n",
        "    p = np.random.random(1).squeeze() #1D entries returned using squeeze\n",
        "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cyeVdMEBbxI",
        "colab_type": "text"
      },
      "source": [
        "Initialize buffer length and define function for sampling experiences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "818-ihe2Bnl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_len = 20000\n",
        "#Buffer is made from a deque - double ended queue\n",
        "exp_buffer = deque(maxlen=buffer_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgGmyHbSBp-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_memories(batch_size):\n",
        "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "    mem = np.array(exp_buffer)[perm_batch]\n",
        "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jrueqb_B4pj",
        "colab_type": "text"
      },
      "source": [
        "Define network hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmyTOREbB_Kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 50\n",
        "batch_size = 25\n",
        "input_shape = (None, 88, 80, 1)\n",
        "learning_rate = 0.001\n",
        "X_shape = (None, 88, 80, 1)\n",
        "discount_factor = 0.97\n",
        "\n",
        "global_step = 0\n",
        "copy_steps = 100\n",
        "steps_train = 4\n",
        "start_steps = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7e2_kk1CENj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = 'logs'\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# define the placeholder for our input i.e game state\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=X_shape)\n",
        "\n",
        "# define a boolean called in_training_model to toggle the training\n",
        "in_training_mode = tf.compat.v1.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-Y86ntCXma",
        "colab_type": "text"
      },
      "source": [
        "Build primary and target Q network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yccs6WS0CbUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_outputs = env.action_space.n\n",
        "\n",
        "#build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "\n",
        "# build our target Q network\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqQVUrp4GaH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the placeholder for  action values\n",
        "\n",
        "X_action = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "Q_action = tf.reduce_sum(input_tensor=targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n",
        "\n",
        "copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKoOm_xBCuop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a placeholder for output i.e action\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))\n",
        "\n",
        "# calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n",
        "\n",
        "# adam optimizer for minimizing the loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "loss_summary = tf.compat.v1.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.compat.v1.summary.merge_all()\n",
        "file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMP1BxSsC51d",
        "colab_type": "text"
      },
      "source": [
        "Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcrIfcd5C8ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    # for each episode\n",
        "    history = []\n",
        "    for i in range(num_episodes):\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        epoch = 0\n",
        "        episodic_reward = 0\n",
        "        actions_counter = Counter() \n",
        "        episodic_loss = []\n",
        "\n",
        "        # while the state is not the terminal state\n",
        "        while not done:\n",
        "\n",
        "           #env.render()\n",
        "        \n",
        "            # get the preprocessed game screen\n",
        "            obs = preprocess_observation(obs)\n",
        "\n",
        "            # feed the game screen and get the Q values for each action,  FEED THE NETWORK BY CALLING THE OUTPUT LAYER\n",
        "            \n",
        "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "            # get the action\n",
        "            action = np.argmax(actions, axis=-1)\n",
        "            actions_counter[str(action)] += 1 \n",
        "\n",
        "            # select the action using epsilon greedy policy\n",
        "            action = epsilon_greedy(action, global_step)\n",
        "            \n",
        "            # now perform the action and move to the next state, next_obs, receive reward\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store this transistion as an experience in the replay buffer! Quite important\n",
        "            exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
        "            \n",
        "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "            if global_step % steps_train == 0 and global_step > start_steps:\n",
        "                \n",
        "                # sample experience, mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n",
        "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
        "\n",
        "                # states\n",
        "                o_obs = [x for x in o_obs]\n",
        "\n",
        "                # next states\n",
        "                o_next_obs = [x for x in o_next_obs]\n",
        "\n",
        "                # next actions\n",
        "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
        "\n",
        "\n",
        "                # discounted reward: these are our Y-values\n",
        "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
        "\n",
        "                # merge all summaries and write to the file\n",
        "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
        "                file_writer.add_summary(mrg_summary, global_step)\n",
        "\n",
        "                # To calculate the loss, we run the previously defined functions mentioned while feeding inputs\n",
        "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
        "                episodic_loss.append(train_loss)\n",
        "            \n",
        "            # after some interval we copy our main Q network weights to target Q network\n",
        "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "                copy_target_to_main.run()\n",
        "                \n",
        "            obs = next_obs\n",
        "            epoch += 1\n",
        "            global_step += 1\n",
        "            episodic_reward += reward\n",
        "        \n",
        "        history.append(episodic_reward)\n",
        "        print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,\"Episode number:\", len(history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTeazUekDEg7",
        "colab_type": "text"
      },
      "source": [
        "Plot the reward distribution accross episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu-Slmj2DDRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHKr6quFEMFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate model on openAi GYM\n",
        "env = wrap_env(gym.make('MsPacman-v0'))\n",
        "observation = env.reset()\n",
        "new_observation = observation\n",
        "\n",
        "prev_input = None\n",
        "done = False\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    while True:\n",
        "      if True: \n",
        "    \n",
        "        #set input to network to be difference image\n",
        "    \n",
        "\n",
        "        obs = preprocess_observation(observation)\n",
        "\n",
        "        # feed the game screen and get the Q values for each action\n",
        "        actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "        # get the action\n",
        "        action = np.argmax(actions, axis=-1)\n",
        "        actions_counter[str(action)] += 1 \n",
        "\n",
        "        # select the action using epsilon greedy policy\n",
        "        action = epsilon_greedy(action, global_step)\n",
        "        env.render()\n",
        "        observation = new_observation        \n",
        "        # now perform the action and move to the next state, next_obs, receive reward\n",
        "        new_observation, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done: \n",
        "          break\n",
        "      \n",
        "    env.close()\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNXKDGf5M1Ep",
        "colab_type": "text"
      },
      "source": [
        "This code was created with help from:\n",
        "\n",
        "Francis, J. 2017. Introduction to reinforcement learning and OpenAI Gym. https://www.oreilly.com/radar/introduction-to-reinforcement-learning-and-openai-gym/\n",
        "\n",
        "Grigsby, J. 2018. Advanced DQNs: Playing Pac-man with Deep Reinforcement Learning. Towards Data Science. https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814\n",
        "\n",
        "Xu, A. X. 2019. Automating Pac-man with Deep Q-learning: An Implementation in Tensorflow. Towards Data Science. \n",
        "https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c"
      ]
    }
  ]
}